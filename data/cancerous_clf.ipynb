{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436fed97-c446-4e3c-a28b-a7e9d932cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047cfe88-670d-4003-bad7-ac6d585ac4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dbbf3e-3a53-491b-ac4e-a83dc7fe1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(file_path='./data/training'):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"Dataset preview:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Basic dataset information\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nColumn information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Class distribution\n",
    "    print(\"\\nClass distribution for isCancerous:\")\n",
    "    print(df['isCancerous'].value_counts())\n",
    "    print(df['isCancerous'].value_counts(normalize=True))\n",
    "    \n",
    "    # Visualize the class distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='isCancerous', data=df)\n",
    "    plt.title('Distribution of Cancerous vs Non-Cancerous Cells')\n",
    "    plt.xlabel('Is Cancerous (0=No, 1=Yes)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    # Examine relationship between cell type and cancer status\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='cellTypeName', hue='isCancerous', data=df)\n",
    "    plt.title('Cell Types by Cancer Status')\n",
    "    plt.xlabel('Cell Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(['Non-Cancerous', 'Cancerous'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39068767-7f40-496f-8560-c77cc07d3ed1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cell_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m load_and_explore_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mload_and_explore_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_explore_data\u001b[39m(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/training\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Display the first few rows\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cell_data.csv'"
     ]
    }
   ],
   "source": [
    "df = load_and_explore_data('cell_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b8a2d-8dbf-49c1-82b8-9ce8a4d94e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_images(dataframe, image_dir):\n",
    "    \"\"\"\n",
    "    Extract features from images.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframe: DataFrame containing image information\n",
    "    image_dir: Directory containing the images\n",
    "    \n",
    "    Returns:\n",
    "    X: Feature matrix\n",
    "    y: Target vector\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Extracting features from images...\")\n",
    "    for index, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "        image_path = os.path.join(image_dir, row['ImageName'])\n",
    "        \n",
    "        try:\n",
    "            # Load image (assuming grayscale medical images)\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # 1. Basic statistical features\n",
    "            mean_intensity = np.mean(img)\n",
    "            std_intensity = np.std(img)\n",
    "            min_intensity = np.min(img)\n",
    "            max_intensity = np.max(img)\n",
    "            \n",
    "            # 2. Histogram features\n",
    "            hist = cv2.calcHist([img], [0], None, [32], [0, 256])\n",
    "            hist_features = hist.flatten() / np.sum(hist)  # Normalize\n",
    "            \n",
    "            # 3. Texture features using edge detection as proxy\n",
    "            edges = cv2.Canny(img, 100, 200)\n",
    "            edge_density = np.sum(edges > 0) / (img.shape[0] * img.shape[1])\n",
    "            \n",
    "            # 4. Shape features\n",
    "            # Apply thresholding\n",
    "            _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Calculate area and perimeter if contours exist\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                area = cv2.contourArea(largest_contour)\n",
    "                perimeter = cv2.arcLength(largest_contour, True)\n",
    "                circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
    "            else:\n",
    "                area = 0\n",
    "                perimeter = 0\n",
    "                circularity = 0\n",
    "            \n",
    "            # Combine all features\n",
    "            image_features = [\n",
    "                mean_intensity, std_intensity, min_intensity, max_intensity,\n",
    "                edge_density, area, perimeter, circularity\n",
    "            ]\n",
    "            \n",
    "            # Add histogram features\n",
    "            image_features.extend(hist_features)\n",
    "            \n",
    "            features.append(image_features)\n",
    "            labels.append(row['isCancerous'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9662d5-4fef-4f4a-a821-8437d2650c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "# Prepare the data for modeling\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, scaler = prepare_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc9c877-d62b-4e32-8c21-942b21636841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"5-Fold CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Cancerous', 'Cancerous'],\n",
    "                yticklabels=['Non-Cancerous', 'Cancerous'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ROC curve for binary classification\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Return the trained model and metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cv_mean': np.mean(cv_scores),\n",
    "        'cv_std': np.std(cv_scores)\n",
    "    }\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bba55-c9f2-4052-8342-cf2915ae157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n---- Logistic Regression ----\")\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_model, log_reg_metrics = evaluate_model(\n",
    "    log_reg, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e854c-98dc-456e-98f3-cb2859063530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n---- Random Forest ----\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_model, rf_metrics = evaluate_model(\n",
    "    rf, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99ebe0-488c-49cf-a3b3-7909897685a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n---- Support Vector Machine ----\")\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "svm_model, svm_metrics = evaluate_model(\n",
    "    svm, X_train_scaled, X_test_scaled, y_train, y_test, \"SVM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983e00f-771d-4776-b08a-faa57efaaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_random_forest(X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n---- Hyperparameter Tuning for Random Forest ----\")\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # For faster execution, you can use a smaller parameter grid\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [100, 200],\n",
    "    #     'max_depth': [None, 20],\n",
    "    #     'min_samples_split': [2, 5]\n",
    "    # }\n",
    "\n",
    "    print(\"Starting grid search (this may take some time)...\")\n",
    "    rf_grid = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "    # Evaluate the tuned model\n",
    "    best_rf = rf_grid.best_estimator_\n",
    "    tuned_rf_model, tuned_rf_metrics = evaluate_model(\n",
    "        best_rf, X_train, X_test, y_train, y_test, \"Tuned Random Forest\"\n",
    "    )\n",
    "    \n",
    "    return best_rf, tuned_rf_metrics\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_rf, tuned_rf_metrics = tune_random_forest(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ca508-8d2a-44af-a9a8-d76114bb2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X):\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(\"This model doesn't support feature importance analysis\")\n",
    "        return\n",
    "    \n",
    "    feature_importances = model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': [f'Feature_{i}' for i in range(X.shape[1])],\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "    plt.title('Top 15 Features by Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance for the tuned Random Forest\n",
    "feature_importance_df = analyze_feature_importance(best_rf, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa905e3-044a-4fa3-959b-8446be1099fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, model_filename=\"cancer_prediction_model.pkl\", scaler_filename=\"cancer_prediction_scaler.pkl\"):\n",
    "    # Save the model\n",
    "    with open(model_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "    # Save the scaler\n",
    "    with open(scaler_filename, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    print(f\"\\nModel saved as '{model_filename}'\")\n",
    "    print(f\"Scaler saved as '{scaler_filename}'\")\n",
    "\n",
    "# Save the best model and scaler\n",
    "save_model(best_rf, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e73dd-1d83-4c6b-82d1-796bb04361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cancer(model, scaler, image_path):\n",
    "    # Load and process the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image {image_path}\")\n",
    "    \n",
    "    # Extract features (same as in training)\n",
    "    mean_intensity = np.mean(img)\n",
    "    std_intensity = np.std(img)\n",
    "    min_intensity = np.min(img)\n",
    "    max_intensity = np.max(img)\n",
    "    \n",
    "    hist = cv2.calcHist([img], [0], None, [32], [0, 256])\n",
    "    hist_features = hist.flatten() / np.sum(hist)\n",
    "    \n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    edge_density = np.sum(edges > 0) / (img.shape[0] * img.shape[1])\n",
    "    \n",
    "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        perimeter = cv2.arcLength(largest_contour, True)\n",
    "        circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
    "    else:\n",
    "        area = 0\n",
    "        perimeter = 0\n",
    "        circularity = 0\n",
    "    \n",
    "    image_features = [\n",
    "        mean_intensity, std_intensity, min_intensity, max_intensity,\n",
    "        edge_density, area, perimeter, circularity\n",
    "    ]\n",
    "    image_features.extend(hist_features)\n",
    "    \n",
    "    # Reshape for scaler\n",
    "    features = np.array(image_features).reshape(1, -1)\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    \n",
    "    # Get probability\n",
    "    probability = model.predict_proba(features_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c23ede-34d8-406d-9f36-9d0189978e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with an actual test image path\n",
    "test_image_path = './cell_images/test_cell.png'\n",
    "\n",
    "# Predict using the saved model (you would first need to load the model)\n",
    "prediction, probability = predict_cancer(best_rf, scaler, test_image_path)\n",
    "\n",
    "print(f\"\\nPrediction for test image: {'Cancerous' if prediction == 1 else 'Non-cancerous'}\")\n",
    "print(f\"Probability of being cancerous: {probability:.4f}\")\n",
    "\n",
    "# Visualize the test image\n",
    "img = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f\"Prediction: {'Cancerous' if prediction == 1 else 'Non-cancerous'} ({probability:.2f})\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34b92f-32e5-453b-bed7-b08f3f82cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(log_reg_metrics, rf_metrics, svm_metrics, tuned_rf_metrics):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create a DataFrame to compare models\n",
    "    models = ['Logistic Regression', 'Random Forest', 'SVM', 'Tuned Random Forest']\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'Accuracy': [log_reg_metrics['accuracy'], rf_metrics['accuracy'], \n",
    "                    svm_metrics['accuracy'], tuned_rf_metrics['accuracy']],\n",
    "        'Precision': [log_reg_metrics['precision'], rf_metrics['precision'], \n",
    "                     svm_metrics['precision'], tuned_rf_metrics['precision']],\n",
    "        'Recall': [log_reg_metrics['recall'], rf_metrics['recall'], \n",
    "                  svm_metrics['recall'], tuned_rf_metrics['recall']],\n",
    "        'F1 Score': [log_reg_metrics['f1'], rf_metrics['f1'], \n",
    "                    svm_metrics['f1'], tuned_rf_metrics['f1']],\n",
    "        'CV Score': [log_reg_metrics['cv_mean'], rf_metrics['cv_mean'], \n",
    "                    svm_metrics['cv_mean'], tuned_rf_metrics['cv_mean']]\n",
    "    })\n",
    "    \n",
    "    # Round to 4 decimal places for readability\n",
    "    metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'CV Score']] = \\\n",
    "        metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'CV Score']].round(4)\n",
    "    \n",
    "    print(metrics_df)\n",
    "    \n",
    "    # Identify best model based on F1 score\n",
    "    best_model_idx = metrics_df['F1 Score'].idxmax()\n",
    "    best_model_name = metrics_df.loc[best_model_idx, 'Model']\n",
    "    best_f1 = metrics_df.loc[best_model_idx, 'F1 Score']\n",
    "    \n",
    "    print(f\"\\nBest performing model: {best_model_name} with F1 Score: {best_f1}\")\n",
    "    \n",
    "    # Plot performance comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting\n",
    "    plot_df = pd.melt(metrics_df, \n",
    "                      id_vars=['Model'], \n",
    "                      value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'CV Score'],\n",
    "                      var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Create the grouped bar chart\n",
    "    sns.barplot(x='Model', y='Value', hue='Metric', data=plot_df)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Summarize the results of all models\n",
    "results_summary = summarize_results(log_reg_metrics, rf_metrics, svm_metrics, tuned_rf_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13343f38-26ff-4291-9618-16532513c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cell_type_cancer_relationship(df):\n",
    "    print(\"\\nAnalyzing correlation between cell type and cancer status:\")\n",
    "    \n",
    "    # Create a cross-tabulation\n",
    "    cross_tab = pd.crosstab(df['cellTypeName'], df['isCancerous'])\n",
    "    print(cross_tab)\n",
    "    \n",
    "    # Add row percentages\n",
    "    cross_tab_pct = pd.crosstab(df['cellTypeName'], df['isCancerous'], normalize='index') * 100\n",
    "    print(\"\\nPercentage of cancerous cells by cell type:\")\n",
    "    print(cross_tab_pct)\n",
    "    \n",
    "    # Visualize the correlation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cross_tab_normalized = cross_tab.div(cross_tab.sum(axis=1), axis=0)\n",
    "    sns.heatmap(cross_tab_normalized, annot=True, cmap='Blues', fmt='.2f')\n",
    "    plt.title('Correlation between Cell Type and Cancer Status')\n",
    "    plt.ylabel('Cell Type')\n",
    "    plt.xlabel('Is Cancerous (0=No, 1=Yes)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Bar chart showing cancer rate by cell type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cross_tab_pct[1].sort_values(ascending=False).plot(kind='bar', color='darkred')\n",
    "    plt.title('Percentage of Cancerous Cells by Cell Type')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.xlabel('Cell Type')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the relationship\n",
    "analyze_cell_type_cancer_relationship(df)\n",
    "\n",
    "print(\"\\nAll code blocks executed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6eba0-47fc-4b0d-9b66-83e8e2c2f3d2",
   "metadata": {},
   "source": [
    "# Models for classifiying isCanerous status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7500c-9c05-4f9e-8c33-afe452a44f9c",
   "metadata": {},
   "source": [
    "Add the description for the following statements: \n",
    "- What is the problem to solve here? Binary classifcaiton\n",
    "- Which metric should I used? (accuracy? weighted F1 score since we are having imbalanced dataset here, we should consider this)\n",
    "- What is the baseline methods?\n",
    "- Which models (**from the paper**) are used to compare with our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd311e76-2014-4649-9320-90dc0f9d5b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
