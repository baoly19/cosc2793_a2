{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64026c2c-1c53-4c8e-b199-88b9a2b3330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6eba0-47fc-4b0d-9b66-83e8e2c2f3d2",
   "metadata": {},
   "source": [
    "# Models for classifiying cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5f841-00c5-4181-b238-ff04527ac400",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7500c-9c05-4f9e-8c33-afe452a44f9c",
   "metadata": {},
   "source": [
    "**Objective**:\n",
    "The objective of this task is to classify cell type data, which consists of four distinct categories: a, b, c, and others. This problem is framed as a multi-class classification problem, where each sample (an image of a cell) must be assigned one of the four labels based on its features. The dataset is imbalanced, meaning certain classes have far fewer samples than others, which introduces challenges in model evaluation.\n",
    "\n",
    "**Metric Selection**:\n",
    "Given the class imbalance present in the dataset, accuracy alone is not a sufficient evaluation metric. In this context, the weighted F1 score is the preferred metric. The F1 score is the harmonic mean of precision and recall, and it balances these two metrics to provide a more comprehensive evaluation.\n",
    "Since we have an imbalanced dataset, the weighted F1 score is particularly useful because it gives more weight to the larger classes while still considering performance on the smaller classes. This helps ensure that the model performs well across all classes, not just the majority class, and avoids the bias that could arise from imbalanced class distributions. The weighted F1 score is calculated by averaging the F1 scores of each class, weighted by the number of instances in that class. This approach ensures that the model’s performance on minority classes is not overlooked.\n",
    "\n",
    "**Baseline Model**:\n",
    "For a baseline model, we will start with a Random Forest classifier. Random Forest is chosen because it is a robust, interpretable model well-suited for classification tasks, and it performs well on medium-sized datasets. Random Forest does not require extensive hyperparameter tuning and can provide a solid baseline for comparison with more advanced models.\n",
    "\n",
    "**Improvement through Neural Networks**:\n",
    "To improve upon the baseline, we will experiment with Neural Networks (NN) and Convolutional Neural Networks (CNNs). Neural networks, particularly CNNs, are highly specialized for image classification tasks as they can automatically learn hierarchical features and patterns from image data. CNNs have been shown to outperform traditional machine learning models in many image-related tasks due to their ability to capture spatial relationships in images. These models are expected to provide a significant boost in performance compared to the Random Forest baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e2ec2-9bfb-46a3-a79d-383d13475577",
   "metadata": {},
   "source": [
    "## Baseline development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd311e76-2014-4649-9320-90dc0f9d5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the random forest classifier, as the image does not represent, so we need to extract the features for classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29d9b4e4-6fbc-455d-80d0-77a79577af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to load and preprocess image data\n",
    "def load_and_extract_features(image_path, size=(27, 27)):\n",
    "    \"\"\"\n",
    "    Load an image, resize it, and extract features using HOG and LBP\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Resize image\n",
    "        # img = img.resize(size)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # If image is grayscale, convert to RGB\n",
    "        if len(img_array.shape) == 2:\n",
    "            img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "            \n",
    "        # Convert to grayscale for feature extraction\n",
    "        gray_img = rgb2gray(img_array)\n",
    "        \n",
    "        # Extract HOG features\n",
    "        hog_features = hog(gray_img, \n",
    "                          orientations=9, \n",
    "                          pixels_per_cell=(8, 8),\n",
    "                          cells_per_block=(2, 2), \n",
    "                          visualize=False)\n",
    "        \n",
    "        # Extract LBP features\n",
    "        lbp_features = local_binary_pattern(gray_img, P=8, R=1)\n",
    "        lbp_hist, _ = np.histogram(lbp_features.flatten(), bins=np.arange(0, 10), density=True)\n",
    "        \n",
    "        # Basic statistical features\n",
    "        red_channel = img_array[:,:,0].flatten()\n",
    "        green_channel = img_array[:,:,1].flatten()\n",
    "        blue_channel = img_array[:,:,2].flatten()\n",
    "        \n",
    "        color_stats = [\n",
    "            np.mean(red_channel), np.std(red_channel),\n",
    "            np.mean(green_channel), np.std(green_channel),\n",
    "            np.mean(blue_channel), np.std(blue_channel)\n",
    "        ]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = np.concatenate([hog_features, lbp_hist, color_stats])\n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        # Return a vector of zeros in case of error\n",
    "        return np.zeros(324) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0afa06a9-d94f-476b-9aec-1266bbc6b8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InstanceID</th>\n",
       "      <th>patientID</th>\n",
       "      <th>ImageName</th>\n",
       "      <th>cellTypeName</th>\n",
       "      <th>cellType</th>\n",
       "      <th>isCancerous</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>combined_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19035</td>\n",
       "      <td>2</td>\n",
       "      <td>19035.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/preprocessed/train/19035.png</td>\n",
       "      <td>0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19036</td>\n",
       "      <td>2</td>\n",
       "      <td>19036.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/preprocessed/train/19036.png</td>\n",
       "      <td>0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19037</td>\n",
       "      <td>2</td>\n",
       "      <td>19037.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/preprocessed/train/19037.png</td>\n",
       "      <td>0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19038</td>\n",
       "      <td>2</td>\n",
       "      <td>19038.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/preprocessed/train/19038.png</td>\n",
       "      <td>0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19039</td>\n",
       "      <td>2</td>\n",
       "      <td>19039.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/preprocessed/train/19039.png</td>\n",
       "      <td>0_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   InstanceID  patientID  ImageName cellTypeName  cellType  isCancerous  \\\n",
       "0       19035          2  19035.png   fibroblast         0            0   \n",
       "1       19036          2  19036.png   fibroblast         0            0   \n",
       "2       19037          2  19037.png   fibroblast         0            0   \n",
       "3       19038          2  19038.png   fibroblast         0            0   \n",
       "4       19039          2  19039.png   fibroblast         0            0   \n",
       "\n",
       "                             ImagePath combined_label  \n",
       "0  ./data/preprocessed/train/19035.png            0_0  \n",
       "1  ./data/preprocessed/train/19036.png            0_0  \n",
       "2  ./data/preprocessed/train/19037.png            0_0  \n",
       "3  ./data/preprocessed/train/19038.png            0_0  \n",
       "4  ./data/preprocessed/train/19039.png            0_0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celltype_data = pd.read_csv('./data/train_cell_type.csv')\n",
    "celltype_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b38f34a-b69d-4a6f-ae9e-65e1c52c443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with shape: (12112, 8)\n",
      "Sample of data:\n",
      "   InstanceID  patientID  ImageName cellTypeName  cellType  isCancerous  \\\n",
      "0       19035          2  19035.png   fibroblast         0            0   \n",
      "1       19036          2  19036.png   fibroblast         0            0   \n",
      "2       19037          2  19037.png   fibroblast         0            0   \n",
      "3       19038          2  19038.png   fibroblast         0            0   \n",
      "4       19039          2  19039.png   fibroblast         0            0   \n",
      "\n",
      "                             ImagePath combined_label  \n",
      "0  ./data/preprocessed/train/19035.png            0_0  \n",
      "1  ./data/preprocessed/train/19036.png            0_0  \n",
      "2  ./data/preprocessed/train/19037.png            0_0  \n",
      "3  ./data/preprocessed/train/19038.png            0_0  \n",
      "4  ./data/preprocessed/train/19039.png            0_0  \n",
      "\n",
      "Cell type distribution:\n",
      "cellType\n",
      "0    3028\n",
      "1    3028\n",
      "3    3028\n",
      "2    3028\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Patient distribution:\n",
      "Total unique patients: 48\n",
      "Average images per patient: 252.33\n",
      "\n",
      "Class distribution per patient (sample):\n",
      "cellType       0      1     2     3\n",
      "patientID                          \n",
      "2           16.0   36.0   0.0  21.0\n",
      "3          106.0  146.0   0.0  30.0\n",
      "4           78.0  162.0   0.0  21.0\n",
      "5           50.0  206.0  35.0  18.0\n",
      "6           18.0  320.0  28.0   3.0\n",
      "\n",
      "Patients with multiple cell types: 48 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load your existing training data\n",
    "celltype_data = pd.read_csv('./data/train_cell_type.csv')\n",
    "print(\"Dataset loaded with shape:\", celltype_data.shape)\n",
    "print(\"Sample of data:\")\n",
    "print(celltype_data.head())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nCell type distribution:\")\n",
    "print(celltype_data['cellType'].value_counts())\n",
    "\n",
    "# Check patient distribution\n",
    "print(\"\\nPatient distribution:\")\n",
    "print(f\"Total unique patients: {celltype_data['patientID'].nunique()}\")\n",
    "print(f\"Average images per patient: {celltype_data.shape[0] / celltype_data['patientID'].nunique():.2f}\")\n",
    "\n",
    "# Check class distribution per patient\n",
    "patient_class_counts = celltype_data.groupby('patientID')['cellType'].value_counts().unstack().fillna(0)\n",
    "print(\"\\nClass distribution per patient (sample):\")\n",
    "print(patient_class_counts.head())\n",
    "\n",
    "# Analyze if patients have multiple cell types\n",
    "patients_with_multiple_types = patient_class_counts[patient_class_counts.sum(axis=1) > 1].shape[0]\n",
    "print(f\"\\nPatients with multiple cell types: {patients_with_multiple_types} ({patients_with_multiple_types/celltype_data['patientID'].nunique()*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6422338-a06a-445e-82ec-de11d8862f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RECOMMENDATION ===\n",
      "Most patients have multiple cell types. A simple train/validation split should be adequate.\n",
      "\n",
      "FINAL RECOMMENDATION: Use a simple patient-stratified train/validation split.\n",
      "\n",
      "NOTES:\n",
      "- Always ensure patients are not split between train and validation sets\n",
      "- Consider class imbalance in your model using class_weight='balanced'\n",
      "- If you're tuning hyperparameters, use nested cross-validation or a separate test set\n"
     ]
    }
   ],
   "source": [
    "def print_recommendation():\n",
    "    y = celltype_data['cellType'].values\n",
    "    patient_ids = celltype_data['patientID'].values\n",
    "    num_classes = len(np.unique(y))\n",
    "    num_patients = len(np.unique(patient_ids))\n",
    "    samples_per_class = np.bincount(y.astype(int))\n",
    "    min_samples = np.min(samples_per_class)\n",
    "    \n",
    "    print(\"\\n=== RECOMMENDATION ===\")\n",
    "    \n",
    "    if num_patients < 10:\n",
    "        print(\"You have a small number of patients (<10). K-fold cross-validation is strongly recommended.\")\n",
    "        recommended = \"k-fold\"\n",
    "    elif min_samples < 20:\n",
    "        print(\"You have few samples (<20) for at least one class. K-fold cross-validation is recommended.\")\n",
    "        recommended = \"k-fold\"\n",
    "    elif patients_with_multiple_types / num_patients > 0.5:\n",
    "        print(\"Most patients have multiple cell types. A simple train/validation split should be adequate.\")\n",
    "        recommended = \"simple\"\n",
    "    else:\n",
    "        print(\"Your dataset size is reasonable. Both approaches are valid.\")\n",
    "        print(\"For more reliable performance estimation, use K-fold cross-validation.\")\n",
    "        print(\"For faster training and iteration, use a simple train/validation split.\")\n",
    "        recommended = \"either\"\n",
    "    \n",
    "    # Final recommendation\n",
    "    if recommended == \"k-fold\":\n",
    "        print(\"\\nFINAL RECOMMENDATION: Use patient-stratified K-fold cross-validation.\")\n",
    "    elif recommended == \"simple\":\n",
    "        print(\"\\nFINAL RECOMMENDATION: Use a simple patient-stratified train/validation split.\")\n",
    "    else:\n",
    "        print(\"\\nFINAL RECOMMENDATION: Either approach is valid. Use simple split for speed, K-fold for reliability.\")\n",
    "    \n",
    "    # Additional notes\n",
    "    print(\"\\nNOTES:\")\n",
    "    print(\"- Always ensure patients are not split between train and validation sets\")\n",
    "    print(\"- Consider class imbalance in your model using class_weight='balanced'\")\n",
    "    print(\"- If you're tuning hyperparameters, use nested cross-validation or a separate test set\")\n",
    "\n",
    "print_recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965f9dab-f10c-44ef-ae11-a9d5792a08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_validation_split(patient_ids, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a single validation split ensuring patients are not split between sets,\n",
    "    while trying to maintain class distribution.\n",
    "    \"\"\"\n",
    "    # Get unique patient IDs\n",
    "    unique_patients = np.unique(patient_ids)\n",
    "    \n",
    "    # Calculate average class per patient (for stratification)\n",
    "    patient_class_mapping = {}\n",
    "    for patient in unique_patients:\n",
    "        patient_mask = patient_ids == patient\n",
    "        patient_classes = y[patient_mask]\n",
    "        # Use most frequent class for this patient\n",
    "        if len(patient_classes) > 0:\n",
    "            patient_class_mapping[patient] = np.bincount(patient_classes.astype(int)).argmax()\n",
    "        else:\n",
    "            patient_class_mapping[patient] = -1  # No data for this patient\n",
    "    \n",
    "    # Create stratification array\n",
    "    patient_strata = np.array([patient_class_mapping[p] for p in unique_patients])\n",
    "    \n",
    "    # Split patients into train and validation groups\n",
    "    patients_train, patients_val = train_test_split(\n",
    "        unique_patients, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=patient_strata if len(np.unique(patient_strata)) > 1 else None\n",
    "    )\n",
    "    \n",
    "    # Create masks for train and validation sets\n",
    "    train_mask = np.isin(patient_ids, patients_train)\n",
    "    val_mask = np.isin(patient_ids, patients_val)\n",
    "    \n",
    "    return train_mask, val_mask, patients_train, patients_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2324c950-56be-4ac3-bdd1-3128fe9a2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for idx, row in celltype_data.iterrows():\n",
    "    try:\n",
    "        image_path = row['ImagePath']\n",
    "        feature_vector = load_and_extract_features(image_path)\n",
    "        if np.isnan(feature_vector).sum() > 0:\n",
    "            print(image_path)\n",
    "        features.append(feature_vector)\n",
    "    except Exception as e:\n",
    "        print(\"can't load image: \" + e)\n",
    "X = np.array(features)\n",
    "print(np.isnan(X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d632c922-45fd-4a8a-a2e9-885864efc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = celltype_data['patientID'].values\n",
    "y = celltype_data['cellType'].values\n",
    "train_mask, val_mask, patients_train, patients_val = create_simple_validation_split(patient_ids, y)\n",
    "X_train, X_val = X[train_mask], X[val_mask]\n",
    "y_train, y_val = y[train_mask], y[val_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b604acef-df3f-4727-8427-4e8bf12af0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set (simulated)...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.26      0.37       874\n",
      "           1       0.51      0.39      0.44       658\n",
      "           2       0.55      0.88      0.68       498\n",
      "           3       0.28      0.48      0.35       505\n",
      "\n",
      "    accuracy                           0.46      2535\n",
      "   macro avg       0.49      0.50      0.46      2535\n",
      "weighted avg       0.51      0.46      0.45      2535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rd_clf = RandomForestClassifier()\n",
    "rd_clf.fit(X_train, y_train)\n",
    "print(\"Evaluating on validation set (simulated)...\")\n",
    "y_pred = rd_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d57fdd-d529-4974-906b-747da09a64b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751d079-8004-49b0-b4ae-4800fc90afc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e62a0-00a5-40c7-bf58-e45f7a0cf2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea05c900-3f92-44a0-a778-34eed6ffdab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (12112, 8)\n",
      "Unique cell types: [0 1 3 2]\n",
      "Number of unique cell types: 4\n",
      "Class distribution:\n",
      "cellType\n",
      "0    3028\n",
      "1    3028\n",
      "3    3028\n",
      "2    3028\n",
      "Name: count, dtype: int64\n",
      "Training set: 8355 samples from 33 patients\n",
      "Validation set: 3757 samples from 15 patients\n",
      "Loading and preprocessing images...\n",
      "Input shape: (8355, 27, 27, 3)\n",
      "Output shape: (8355, 4)\n",
      "\n",
      "\n",
      "==== TRAINING CNN MODEL ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cosc2820/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_39          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_40          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_44          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,028</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_39          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_40          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,179,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_44          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m1,028\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,322,660</span> (5.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,322,660\u001b[0m (5.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,321,508</span> (5.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,321,508\u001b[0m (5.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m260/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.4427 - precision_2: 0.5279 - recall_2: 0.4443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - loss: 1.4405 - precision_2: 0.5283 - recall_2: 0.4444 - val_loss: 4.3398 - val_precision_2: 0.3951 - val_recall_2: 0.3167 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0200 - precision_2: 0.6361 - recall_2: 0.4877"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.0200 - precision_2: 0.6361 - recall_2: 0.4877 - val_loss: 2.4941 - val_precision_2: 0.4524 - val_recall_2: 0.3854 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9213 - precision_2: 0.6840 - recall_2: 0.4940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.9213 - precision_2: 0.6840 - recall_2: 0.4940 - val_loss: 1.1093 - val_precision_2: 0.5708 - val_recall_2: 0.3883 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.8690 - precision_2: 0.7121 - recall_2: 0.4988 - val_loss: 1.1651 - val_precision_2: 0.6022 - val_recall_2: 0.4307 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.8532 - precision_2: 0.7161 - recall_2: 0.4987 - val_loss: 1.3191 - val_precision_2: 0.5400 - val_recall_2: 0.3503 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.8204 - precision_2: 0.7336 - recall_2: 0.5184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - loss: 0.8204 - precision_2: 0.7335 - recall_2: 0.5184 - val_loss: 1.0278 - val_precision_2: 0.5835 - val_recall_2: 0.5134 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 0.8086 - precision_2: 0.7358 - recall_2: 0.5212 - val_loss: 1.0420 - val_precision_2: 0.6162 - val_recall_2: 0.4285 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 0.7774 - precision_2: 0.7473 - recall_2: 0.5333 - val_loss: 1.1598 - val_precision_2: 0.5402 - val_recall_2: 0.4128 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.7671 - precision_2: 0.7559 - recall_2: 0.5520 - val_loss: 1.1300 - val_precision_2: 0.6463 - val_recall_2: 0.4985 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.7564 - precision_2: 0.7480 - recall_2: 0.5617 - val_loss: 1.0409 - val_precision_2: 0.6277 - val_recall_2: 0.5116 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.7150 - precision_2: 0.7684 - recall_2: 0.5903 - val_loss: 1.0534 - val_precision_2: 0.6448 - val_recall_2: 0.4706 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.6817 - precision_2: 0.7810 - recall_2: 0.6117 - val_loss: 1.0422 - val_precision_2: 0.6187 - val_recall_2: 0.4940 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.6290 - precision_2: 0.7894 - recall_2: 0.6439 - val_loss: 1.2765 - val_precision_2: 0.5659 - val_recall_2: 0.4482 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 0.6044 - precision_2: 0.7967 - recall_2: 0.6705 - val_loss: 1.2615 - val_precision_2: 0.5985 - val_recall_2: 0.4972 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 0.6128 - precision_2: 0.7946 - recall_2: 0.6784 - val_loss: 1.4572 - val_precision_2: 0.5601 - val_recall_2: 0.4908 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.5377 - precision_2: 0.8182 - recall_2: 0.7228 - val_loss: 1.7518 - val_precision_2: 0.4582 - val_recall_2: 0.4030 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 251\u001b[0m\n\u001b[1;32m    249\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m build_cnn_model()\n\u001b[1;32m    250\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m--> 251\u001b[0m cnn_model, cnn_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Create and train FCNN model\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==== TRAINING FULLY CONNECTED NEURAL NETWORK ====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 190\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, model_name, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m    180\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    181\u001b[0m     X_train, y_train,\n\u001b[1;32m    182\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[43mplot_training_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    193\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
      "Cell \u001b[0;32mIn[45], line 224\u001b[0m, in \u001b[0;36mplot_training_history\u001b[0;34m(history, model_name)\u001b[0m\n\u001b[1;32m    221\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Plot Precision\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Precision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    225\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_precision\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Precision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    226\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Precision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'precision'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load and prepare the data\n",
    "celltype_data = pd.read_csv('./data/train_cell_type.csv')\n",
    "print(f\"Dataset shape: {celltype_data.shape}\")\n",
    "\n",
    "# Explore unique cell types for our multiclass classification\n",
    "unique_cell_types = celltype_data['cellType'].unique()\n",
    "print(f\"Unique cell types: {unique_cell_types}\")\n",
    "print(f\"Number of unique cell types: {len(unique_cell_types)}\")\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = celltype_data['cellType'].value_counts()\n",
    "print(\"Class distribution:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Using the existing cellType values (0,1,2,3) instead of encoding\n",
    "n_classes = len(unique_cell_types)\n",
    "\n",
    "# Create patient-based train-validation split using the provided function\n",
    "patient_ids = celltype_data['patientID'].values\n",
    "labels = celltype_data['cellType'].values\n",
    "\n",
    "# Split into train (80%) and validation (20%)\n",
    "train_mask, val_mask, train_patients, val_patients = create_simple_validation_split(\n",
    "    patient_ids, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataframes for each set\n",
    "train_df = celltype_data[train_mask]\n",
    "val_df = celltype_data[val_mask]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples from {len(train_patients)} patients\")\n",
    "print(f\"Validation set: {len(val_df)} samples from {len(val_patients)} patients\")\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images(dataframe, img_size=(27, 27)):\n",
    "    images = []\n",
    "    for img_path in dataframe['ImagePath']:\n",
    "        # Check if path exists and is valid\n",
    "        if os.path.exists(img_path):\n",
    "            img = load_img(img_path, color_mode='rgb', target_size=img_size)\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "        else:\n",
    "            print(f\"Warning: Image not found at {img_path}\")\n",
    "            # Add a blank image as a placeholder\n",
    "            images.append(np.zeros((img_size[0], img_size[1], 3)))\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "# Load images\n",
    "print(\"Loading and preprocessing images...\")\n",
    "X_train = load_images(train_df)\n",
    "X_val = load_images(val_df)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# Get labels\n",
    "y_train = train_df['cellType'].values\n",
    "y_val = val_df['cellType'].values\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Output shape: {y_train_onehot.shape}\")\n",
    "\n",
    "# ======================================================\n",
    "# 1. CNN MODEL IMPLEMENTATION\n",
    "# ======================================================\n",
    "\n",
    "def build_cnn_model(input_shape=(27, 27, 3), n_classes=n_classes):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten and Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ======================================================\n",
    "# 2. FULLY CONNECTED NEURAL NETWORK IMPLEMENTATION\n",
    "# ======================================================\n",
    "\n",
    "def build_fcnn_model(input_shape=(27, 27, 3), n_classes=n_classes):\n",
    "    # Calculate the flattened input size\n",
    "    flat_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Flatten the input images\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        \n",
    "        # First dense layer\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Second dense layer\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Third dense layer\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ======================================================\n",
    "# MODEL TRAINING AND EVALUATION FUNCTION\n",
    "# ======================================================\n",
    "\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "    \"\"\"Train and evaluate a model with standard callbacks\"\"\"\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[Precision(), Recall()]\n",
    "    )\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6, monitor='val_loss'),\n",
    "        ModelCheckpoint(f'best_{model_name}.h5', save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, model_name)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f\"{model_name} Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"{model_name} Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=range(n_classes),\n",
    "                yticklabels=range(n_classes))\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot Precision\n",
    "    ax1.plot(history.history['precision'], label='Train Precision')\n",
    "    ax1.plot(history.history['val_precision'], label='Val Precision')\n",
    "    ax1.set_title(f'{model_name} Precision')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(loc='lower right')\n",
    "\n",
    "    # Plot Recall\n",
    "    ax2.plot(history.history['recall'], label='Train Recall')\n",
    "    ax2.plot(history.history['val_recall'], label='Val Recall')\n",
    "    ax2.set_title(f'{model_name} Recall')\n",
    "    ax2.set_ylabel('Recall')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# TRAIN BOTH MODELS\n",
    "# ======================================================\n",
    "\n",
    "# Create and train CNN model\n",
    "print(\"\\n\\n==== TRAINING CNN MODEL ====\")\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_model.summary()\n",
    "cnn_model, cnn_history = train_and_evaluate_model(\n",
    "    cnn_model, 'CNN', X_train, y_train_onehot, X_val, y_val_onehot\n",
    ")\n",
    "\n",
    "# Create and train FCNN model\n",
    "print(\"\\n\\n==== TRAINING FULLY CONNECTED NEURAL NETWORK ====\")\n",
    "fcnn_model = build_fcnn_model()\n",
    "fcnn_model.summary()\n",
    "fcnn_model, fcnn_history = train_and_evaluate_model(\n",
    "    fcnn_model, 'FCNN', X_train, y_train_onehot, X_val, y_val_onehot\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# HYPERPARAMETER TUNING RECOMMENDATIONS\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\\n==== HYPERPARAMETER TUNING RECOMMENDATIONS ====\")\n",
    "print(\"\"\"\n",
    "Hyperparameter tuning recommendations for cell type classification:\n",
    "\n",
    "1. Learning Rate:\n",
    "   - Initial value: 0.001 (current setting)\n",
    "   - Tuning range: [0.0001, 0.01]\n",
    "   - Justification: The learning rate controls step size during optimization. \n",
    "     Too high may cause divergence, too low leads to slow convergence. The optimal \n",
    "     learning rate depends on the specific dataset characteristics and model architecture.\n",
    "   - Implementation: Use ReduceLROnPlateau callback to dynamically reduce learning rate \n",
    "     when validation metrics plateau.\n",
    "\n",
    "2. Batch Size:\n",
    "   - Initial value: 32 (current setting)\n",
    "   - Tuning range: [16, 64]\n",
    "   - Justification: Smaller batch sizes provide more parameter updates and potentially better \n",
    "     generalization but slower training. Larger batch sizes enable better gradient estimates \n",
    "     and faster training but may lead to poorer generalization.\n",
    "   - Implementation: Grid search different batch sizes and measure validation performance.\n",
    "\n",
    "3. Network Depth:\n",
    "   - CNN: Test with 2-4 convolutional blocks\n",
    "   - FCNN: Test with 2-5 dense layers\n",
    "   - Justification: Deeper networks can learn more complex representations but require more \n",
    "     data and are prone to overfitting. For 27x27 images, extremely deep networks may be \n",
    "     unnecessary and could lead to overfitting.\n",
    "\n",
    "4. Dropout Rate:\n",
    "   - Current values: 0.25 (convolutional layers), 0.5 (dense layers)\n",
    "   - Tuning range: [0.1, 0.5]\n",
    "   - Justification: Dropout is a regularization technique to prevent overfitting. The optimal \n",
    "     rate depends on model complexity and dataset size. Early layers typically need less \n",
    "     dropout than later layers.\n",
    "\n",
    "5. Number of Filters/Neurons:\n",
    "   - CNN filters: Test [16, 32, 64] for first layer, doubling in each subsequent layer\n",
    "   - FCNN neurons: Test different architectures like [512, 256], [1024, 512, 256], etc.\n",
    "   - Justification: The number of filters/neurons controls model capacity. Too few may \n",
    "     result in underfitting, while too many can lead to overfitting and increased computational cost.\n",
    "\n",
    "6. Data Augmentation Parameters:\n",
    "   - Rotation range: 15-30 degrees\n",
    "   - Width/height shift: 0.1-0.2\n",
    "   - Zoom range: 0.1-0.15\n",
    "   - Horizontal/vertical flip: True/False depending on cell orientation importance\n",
    "   - Justification: Data augmentation increases effective training set size and improves \n",
    "     model generalization. The parameters should be tuned to match expected variations in the data \n",
    "     while preserving class-specific features.\n",
    "\n",
    "7. Early Stopping Patience:\n",
    "   - Current value: 10 epochs\n",
    "   - Tuning range: [5, 15]\n",
    "   - Justification: Patience determines how many epochs to wait for improvement before stopping \n",
    "     training. Too short may stop training prematurely, while too long wastes computational resources.\n",
    "\n",
    "8. Optimizer:\n",
    "   - Current choice: Adam\n",
    "   - Alternatives: RMSprop, SGD with momentum\n",
    "   - Justification: Different optimizers have different convergence properties. Adam generally \n",
    "     works well for most problems, but alternatives may perform better in specific cases.\n",
    "\n",
    "Recommended Tuning Approach:\n",
    "1. Start with learning rate and batch size tuning as these often have the largest impact\n",
    "2. Proceed to network architecture tuning (depth, width)\n",
    "3. Fine-tune regularization parameters (dropout)\n",
    "4. Optimize data augmentation parameters\n",
    "5. Consider ensemble methods combining CNN and FCNN predictions for potentially better performance\n",
    "\"\"\")\n",
    "\n",
    "# Function to visualize model architecture\n",
    "def visualize_model_architecture(model, model_name):\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(model, to_file=f'{model_name}_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "    print(f\"Model architecture saved as {model_name}_architecture.png\")\n",
    "\n",
    "# Uncomment to visualize model architectures\n",
    "# visualize_model_architecture(cnn_model, 'CNN')\n",
    "# visualize_model_architecture(fcnn_model, 'FCNN')\n",
    "\n",
    "# ======================================================\n",
    "# DATA AUGMENTATION IMPLEMENTATION (FOR REFERENCE)\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\\n==== DATA AUGMENTATION IMPLEMENTATION ====\")\n",
    "print(\"\"\"\n",
    "# Example implementation of data augmentation:\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,           # Random rotations (0-20 degrees)\n",
    "    width_shift_range=0.15,      # Random horizontal shifts\n",
    "    height_shift_range=0.15,     # Random vertical shifts\n",
    "    zoom_range=0.15,             # Random zoom\n",
    "    horizontal_flip=True,        # Random horizontal flips\n",
    "    vertical_flip=False,         # No vertical flips (cell orientation may matter)\n",
    "    fill_mode='nearest'          # Strategy for filling points outside the input boundaries\n",
    ")\n",
    "\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Train using the generator\n",
    "model.fit(\n",
    "    datagen.flow(X_train, y_train_onehot, batch_size=32),\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    steps_per_epoch=len(X_train) // 32,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Save final models\n",
    "cnn_model.save('final_cnn_model.h5')\n",
    "fcnn_model.save('final_fcnn_model.h5')\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61330e1e-0f6d-448f-bec4-ea5857f79f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16297370-0e82-466b-bc9a-25b63ea29a5f",
   "metadata": {},
   "source": [
    "## Using extra data to improve the current accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a561e-0415-4cb0-b145-0d17da450f5f",
   "metadata": {},
   "source": [
    "Analysis on the given data: \n",
    "The given data does not have the labels for the cell type but only with the cancerous status. So, one possible way is to train the embeddings via the encoder/decoder neural network (i.e. similar to what BERT do), then fine tuned on the given dataset. \n",
    "\n",
    "Running the transfer learning: https://www.tensorflow.org/tutorials/images/transfer_learning#create_the_base_model_from_the_pre-trained_convnets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf84ee-5c78-4b31-adfc-2f992083f206",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e16480-bd8b-4902-a00e-7081f72716e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
